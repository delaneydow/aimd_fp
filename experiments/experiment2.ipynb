{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4c899b-5410-47f2-aaa0-e12659ceef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+cu128\n",
      "CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Limit threads & CPU-only\n",
    "# -------------------------------\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # disable GPU\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e19601-088c-417b-a33a-7145be774a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Tiny CPU-friendly 2D UNet Training\n",
    "# Low-memory version\n",
    "# =====================================\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80ae76f-cd08-4d28-a7e5-e049e230040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Device\n",
    "# -------------------------------\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3aa172-0f18-4245-9db9-a847ecb3fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Dataset helper (pairs only)\n",
    "# -------------------------------\n",
    "def collect_image_mask_pairs(root_dir):\n",
    "    pairs = []\n",
    "    for patient_dir in os.listdir(root_dir):\n",
    "        full_patient_dir = os.path.join(root_dir, patient_dir)\n",
    "        if not os.path.isdir(full_patient_dir):\n",
    "            continue\n",
    "        files = sorted(os.listdir(full_patient_dir))\n",
    "        images = [f for f in files if not f.endswith(\"_mask.tif\")]\n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(full_patient_dir, img_name)\n",
    "            mask_name = img_name.replace(\".tif\", \"_mask.tif\")\n",
    "            mask_path = os.path.join(full_patient_dir, mask_name)\n",
    "            if os.path.exists(mask_path):\n",
    "                pairs.append((img_path, mask_path))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3167616-9a00-4e7c-82c3-ea2f52d23a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. On-the-fly Dataset\n",
    "# -------------------------------\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class SliceDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        # collect all image/mask pairs once\n",
    "        self.samples = []\n",
    "        for patient_dir in os.listdir(root_dir):\n",
    "            full_patient_dir = os.path.join(root_dir, patient_dir)\n",
    "            if not os.path.isdir(full_patient_dir):\n",
    "                continue\n",
    "            files = sorted(os.listdir(full_patient_dir))\n",
    "            images = [f for f in files if not f.endswith(\"_mask.tif\")]\n",
    "            for img_name in images:\n",
    "                img_path = os.path.join(full_patient_dir, img_name)\n",
    "                mask_name = img_name.replace(\".tif\", \"_mask.tif\")\n",
    "                mask_path = os.path.join(full_patient_dir, mask_name)\n",
    "                if os.path.exists(mask_path):\n",
    "                    self.samples.append((img_path, mask_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.samples[idx]\n",
    "\n",
    "        # Load image/mask lazily, convert to float32, normalize\n",
    "        img = np.array(Image.open(img_path), dtype=np.float32) / 255.0\n",
    "        mask = np.array(Image.open(mask_path), dtype=np.float32) / 255.0\n",
    "\n",
    "        # If 3D (H, W, D), pick middle slice\n",
    "        if img.ndim == 3:\n",
    "            mid = img.shape[2] // 2\n",
    "            img = img[:, :, mid]\n",
    "            mask = mask[:, :, mid]\n",
    "\n",
    "        # Add channel dimension for Conv2d\n",
    "        img = np.expand_dims(img, 0)\n",
    "        mask = np.expand_dims(mask, 0)\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(img, dtype=torch.float32),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.float32),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70de028e-f445-4a0e-8432-0a6ba66129b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Tiny UNet\n",
    "# -------------------------------\n",
    "class UNetTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(1, 8, 3, padding=1), nn.ReLU(),\n",
    "                                  nn.Conv2d(8, 8, 3, padding=1), nn.ReLU())\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(8, 16, 3, padding=1), nn.ReLU(),\n",
    "                                  nn.Conv2d(16, 16, 3, padding=1), nn.ReLU())\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec1 = nn.Sequential(nn.Conv2d(24, 8, 3, padding=1), nn.ReLU(),\n",
    "                                  nn.Conv2d(8, 8, 3, padding=1), nn.ReLU())\n",
    "        self.outc = nn.Conv2d(8, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        d1 = self.up(e2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        out = self.dec1(d1)\n",
    "        return self.outc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac117b5-823a-4713-9b2e-fd5f2c1a7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Dice loss / metric\n",
    "# -------------------------------\n",
    "def dice_loss(pred, target, eps=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return 1 - (2.0 * intersection + eps) / (union + eps)\n",
    "\n",
    "def dice_metric(pred, target, eps=1e-6):\n",
    "    pred = (torch.sigmoid(pred) > 0.5).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    return (2.0 * intersection + eps) / (union + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f4a61a-ad02-4548-9886-a47cf43b053a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "listdir: path should be string, bytes, os.PathLike, integer or None, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m data_root = \u001b[33m\"\u001b[39m\u001b[33m/home/jovyan/.cache/kagglehub/datasets/mateuszbuda/lgg-mri-segmentation/versions/2/kaggle_3m\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m pairs = collect_image_mask_pairs(data_root)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset = \u001b[43mSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset) == \u001b[32m0\u001b[39m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo image/mask pairs found. Check paths!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mSliceDataset.__init__\u001b[39m\u001b[34m(self, root_dir)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# collect all image/mask pairs once\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.samples = []\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m patient_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     15\u001b[39m         full_patient_dir = os.path.join(root_dir, patient_dir)\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(full_patient_dir):\n",
      "\u001b[31mTypeError\u001b[39m: listdir: path should be string, bytes, os.PathLike, integer or None, not list"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6. Data\n",
    "# -------------------------------\n",
    "data_root = \"/home/jovyan/.cache/kagglehub/datasets/mateuszbuda/lgg-mri-segmentation/versions/2/kaggle_3m\"\n",
    "pairs = collect_image_mask_pairs(data_root)\n",
    "dataset = SliceDataset(pairs)\n",
    "if len(dataset) == 0:\n",
    "    raise RuntimeError(\"No image/mask pairs found. Check paths!\")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013fec46-2df4-43ea-85d0-2b436f1d910f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SliceDataset' object has no attribute 'samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m model.train()\n\u001b[32m     13\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/blueberry/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/blueberry/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/blueberry/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/blueberry/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSliceDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     img_path, mask_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msamples\u001b[49m[idx]\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Load image and mask\u001b[39;00m\n\u001b[32m     15\u001b[39m     img = np.array(Image.open(img_path), dtype=np.float32) / \u001b[32m255.0\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'SliceDataset' object has no attribute 'samples'"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 7. Model / optimizer\n",
    "# -------------------------------\n",
    "model = UNetTiny().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Training loop\n",
    "# -------------------------------\n",
    "max_epochs = 3  # keep tiny for testing\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, masks in train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = dice_loss(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_dice = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            val_dice += dice_metric(outputs, masks)\n",
    "    val_dice /= len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "    # Save checkpoint each epoch\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"checkpoints/unet_epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05beb642-8f4b-4f1f-8918-890e4cd019a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blueberry-yolo)",
   "language": "python",
   "name": "blueberry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
